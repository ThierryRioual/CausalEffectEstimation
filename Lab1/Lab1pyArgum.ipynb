{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATE computations from Baysian Networks in RCTs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to study the capabilities of Bayesian Networks for computing Average Treatment Effects (ATE) in Randomized Control Trials (RCT) under the Neyman-Rubin potential outcome framework.\n",
    "\n",
    "Consider a set of $n$ independent and identically distributed subjects. an observation on the $i$-th subject is given by the tuple $(T_i, X_i, Y_i)$ where:\n",
    "* $ T_i $ taking values in $\\{0,1\\}$ is a binary random variable representing the treatment.\n",
    "* $ X_i $ is the covariate vector.\n",
    "* $ Y_i = T_i Y_i(1) + (1-T_i)Y_i(0)$ is the outcome of the treatment on the $i$-th subject, with $Y_i(1)$ and $Y_i(0)$ representing the treated and untreated outcomes, respectively. \n",
    "\n",
    "We are interested in quantifying the effect of a given treatment on the population, namely the quantity $ \\Delta_i = Y_i(1) - Y_i(0) $. Althought this number cannot be directed calculated due to the presence of counterfactuals, there exists methods for approximating its expected value, the Avereage Treatment Effect: $$ \\tau = \\mathbb{E}\\left[\\frac{1}{n}\\sum_{i=1}^n \\Delta_i\\right] = \\mathbb{E}[Y(1)] - \\mathbb{E}[Y(0)]$$\n",
    "\n",
    "To achive this, we suppose the Stable-Unit-Treatment-Value Assumption (SUTVA) is verified and further assume ignorability between the observations: \n",
    "* $Y_i = Y_i(T_i)$ (SUTVA)\n",
    "* $T_i \\perp\\!\\!\\!\\perp \\{Y_i(0), Y_i(1)\\}$ (Ignorability)\n",
    "\n",
    "We will proceed to present estimators of $\\tau$ using Baysian Networks on generated and real data through three different methods:\n",
    "* \"Exact\" Computation\n",
    "* Parameter Learning\n",
    "* Structure Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyAgrum as gum\n",
    "import pyAgrum.skbn as skbn\n",
    "import pyAgrum.lib.notebook as gnb\n",
    "import pyAgrum.lib.explain as gexpl\n",
    "\n",
    "from scipy.stats import norm, gaussian_kde\n",
    "from scipy.integrate import quad\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-whitegrid\")\n",
    "plt.rcParams.update({'font.size': 6, 'font.family': 'DejaVu Sans'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Generated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first consider two generative models in this notebook:\n",
    "\n",
    "* A linear generative model described by the equation:\n",
    "$$ Y = 3X_1 + 2X_2 -2X_3 -0.8X_4 + T(2X_1 + 5X_3 +3X_4) $$\n",
    "\n",
    "* And a non-linear generative model described by the equation:\n",
    "$$ Y = 3X_1 + 2X_2^2 -2X_3 -0.8X_4 +10T $$\n",
    "\n",
    "\n",
    "Where $ (X_1,X_2,X_3,X_4) \\sim \\mathcal{N}_4((1,1,1,1), I_4) $, $T \\sim \\mathcal{Ber}(1/2)$ and $ (X_1,X_2,X_3,X_4,T) $ are jointly independent in both of the models.\n",
    "\n",
    "Data from the models can be generated by the functions given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_simulation(n : int, sigma : float) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Returns n observations from the linear model with normally distributed\n",
    "  noise with expected value 0 and standard deviation sigma.\n",
    "  \"\"\"\n",
    "\n",
    "  X1 = np.random.normal(1, 1, n)\n",
    "  X2 = np.random.normal(1, 1, n)\n",
    "  X3 = np.random.normal(1, 1, n)\n",
    "  X4 = np.random.normal(1, 1, n)\n",
    "  epsilon = np.random.normal(0, sigma, n)\n",
    "  T=np.random.binomial(1, 0.5, n)\n",
    "  Y= 3*X1+2*X2-2*X3-0.8*X4+T*(2*X1+5*X3+3*X4)+epsilon\n",
    "  d=np.array([T,X1,X2,X3,X4,Y])\n",
    "  df_data = pd.DataFrame(data=d.T,columns=['T','X1','X2','X3','X4','Y'])\n",
    "  df_data[\"T\"] = df_data[\"T\"].astype(int)\n",
    "\n",
    "  return df_data\n",
    "\n",
    "def non_linear_simulation(n : int, sigma : float) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Returns n observations from the non-linear model with normally distributed\n",
    "  noise with expected value 0 and standard deviation sigma.\n",
    "  \"\"\"\n",
    "\n",
    "  X1 = np.random.normal(1, 1, n)\n",
    "  X2 = np.random.normal(1, 1, n)\n",
    "  X3 = np.random.normal(1, 1, n)\n",
    "  X4 = np.random.normal(1, 1, n)\n",
    "  epsilon = np.random.normal(0, sigma, n)\n",
    "  T=np.random.binomial(1, 0.5, n)\n",
    "  Y= 3*X1+ 2*X2**2-2*X3-0.8*X4+10*T+epsilon\n",
    "  d=np.array([T,X1,X2,X3,X4,Y])\n",
    "  df_data = pd.DataFrame(data=d.T,columns=['T','X1','X2','X3','X4','Y']) \n",
    "  df_data[\"T\"] = df_data[\"T\"].astype(int)\n",
    "\n",
    "  return df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the expected values of $Y(0)$ and $Y(1)$ can be explicitly calculated, providing us the theoretical ATE which enables performance evaluations of the estimators.\n",
    "\n",
    "Both models have an ATE of $ \\tau = 10 $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computations of the theoretical distributions of Y0 and Y1 given by\n",
    "# the equations of Y\n",
    "\n",
    "X = np.linspace(-20, 40, 120)\n",
    "dx = X[1] - X[0]\n",
    "\n",
    "# Linear model\n",
    "\n",
    "lin_y0_mean, lin_y0_var = (2.2, 17.64)\n",
    "lin_y1_mean, lin_y1_var = (12.2, 42.84)\n",
    "lin_y0 = norm(loc=lin_y0_mean, scale=np.sqrt(lin_y0_var)).pdf(X)\n",
    "lin_y1 = norm(loc=lin_y1_mean, scale=np.sqrt(lin_y1_var)).pdf(X)\n",
    "lin_pdf_df = pd.DataFrame(data={\"y0\": lin_y0, \"y1\": lin_y1}, index=X)\n",
    "\n",
    "# Non Linear model\n",
    "\n",
    "def twoX2squared_func(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (norm(1, 1).pdf(np.sqrt(x/2.0)) +\n",
    "                norm(1, 1).pdf(-np.sqrt(x/2.0))) / (4.0*np.sqrt(x))\n",
    "\n",
    "def convolve(f, g):\n",
    "    return (lambda t: quad((lambda x: f(t-x)*g(x)), -np.inf, np.inf))\n",
    "\n",
    "nl_y0_norm_mean, nl_y0_norm_var = (0.2, 13.64)\n",
    "nl_y1_norm_mean, nl_y1_norm_var = (10.2, 13.64)\n",
    "nl_y0_norm = norm(loc=nl_y0_norm_mean, scale=np.sqrt(nl_y0_norm_var)).pdf\n",
    "nl_y1_norm = norm(loc=nl_y1_norm_mean, scale=np.sqrt(nl_y1_norm_var)).pdf\n",
    "nl_y0_func = convolve(nl_y0_norm, twoX2squared_func)\n",
    "nl_y1_func = convolve(nl_y1_norm, twoX2squared_func)\n",
    "nl_y0 = list()\n",
    "nl_y1 = list()\n",
    "\n",
    "for x in X:\n",
    "    nl_y0.append(nl_y0_func(x)[0])\n",
    "    nl_y1.append(nl_y1_func(x)[0])\n",
    "\n",
    "nl_y0, nl_y1 = (np.array(nl_y0), np.array(nl_y1))\n",
    "nl_y0, nl_y1 = (nl_y0/(nl_y0.sum()*dx), nl_y1/(nl_y1.sum()*dx))\n",
    "\n",
    "nl_pdf_df = pd.DataFrame(data={\"y0\": nl_y0, \"y1\": nl_y1}, index=X)\n",
    "\n",
    "# Runtime ~ 1m30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distributions\n",
    "\n",
    "plt.subplots(figsize=(7, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.plot(X, lin_y0, color=\"tab:blue\", label=\"$Y(0)$ pdf\")\n",
    "plt.plot(X, lin_y1, color=\"tab:orange\", label=\"$Y(1)$ pdf\")\n",
    "plt.legend()\n",
    "plt.title(\"Linear model Y distributions\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.plot(X, nl_y0, color=\"tab:blue\", label=\"$Y(0)$ pdf\")\n",
    "plt.plot(X, nl_y1, color=\"tab:orange\", label=\"$Y(1)$ pdf\")\n",
    "plt.legend()\n",
    "plt.title(\"Non-linear model Y distributions\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of generated data used for Learning\n",
    "\n",
    "lin_df = linear_simulation(10000, 1.0)\n",
    "nl_df = non_linear_simulation(10000, 1.0)\n",
    "\n",
    "plt.subplots(figsize=(7, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.hist(lin_df[lin_df[\"T\"] == 0][\"Y\"], bins=60, density=True,\n",
    "         alpha=0.5, color=\"tab:blue\", label=\"Generated $Y(0)$ data\")\n",
    "plt.plot(lin_pdf_df[\"y0\"], color=\"tab:blue\", label=\"$Y(0)$\", linewidth=1)\n",
    "plt.hist(lin_df[lin_df[\"T\"] == 1][\"Y\"], bins=60, density=True,\n",
    "         alpha=0.5, color=\"tab:orange\", label=\"Generated $Y(1)$ data\")\n",
    "plt.plot(lin_pdf_df[\"y1\"], color=\"tab:orange\", label=\"$Y(1)$\", linewidth=1)\n",
    "plt.title(\"Linear Model Distributions of generated $Y$ data\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.hist(nl_df[nl_df[\"T\"] == 0][\"Y\"], bins=60, density=True,\n",
    "         alpha=0.5, color=\"tab:blue\", label=\"Generated $Y(0)$ data\")\n",
    "plt.plot(nl_pdf_df[\"y0\"], color=\"tab:blue\", label=\"$Y(0)$\", linewidth=1)\n",
    "plt.hist(nl_df[nl_df[\"T\"] == 1][\"Y\"], bins=60, density=True,\n",
    "         alpha=0.5, color=\"tab:orange\", label=\"Generated $Y(1)$ data\")\n",
    "plt.plot(nl_pdf_df[\"y1\"], color=\"tab:orange\", label=\"$Y(1)$\", linewidth=1)\n",
    "plt.title(\"Non-Linear Model Distributions of generated $Y$ data\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y Expressions\n",
    "\n",
    "lin_expr = \"3*X1 + 2*X2 - 2*X3 - 0.8*X4 + T*(2*X1 + 5*X3 + 3*X4)\"\n",
    "nl_expr = \"3*X1 + 2*(X2*X2) - 2*X3 - 0.8*X4 + 10*T\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - \"Exact\" Computation\n",
    "\n",
    "Exact theoretical expected values can be calculated using Bayesian Networks by inputting the data-generating distribution directly into the network. However, since pyAgrum does not support continuous variables as of July 2024, a discretization of continuous distributions is necessary. Consequently, the calculated value will not be exact in a strict sense, but with a sufficient number of discrete states, a close approximation can be achieved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definitions of functions used in this section\n",
    "\n",
    "def getStringIntervalMean(interval_string : str) -> float:\n",
    "    \"\"\"\n",
    "    Returns the mean of a interval casted as string (e.g. [1.5, 2.9[).\n",
    "    \"\"\"\n",
    "\n",
    "    separator = 0\n",
    "    start = \"\"\n",
    "    end = \"\"\n",
    "    for c in interval_string:\n",
    "        if str.isdecimal(c) or c in {\"-\", \".\"}:\n",
    "            if separator == 1:\n",
    "                start += c\n",
    "            else:\n",
    "                end += c\n",
    "        else:\n",
    "            separator += 1\n",
    "    start = float(start)\n",
    "    end = float(end)\n",
    "\n",
    "    return (start + end)/2.0\n",
    "\n",
    "def getY(bn : gum.BayesNet) -> tuple[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Returns the estimation of outcomes Y(0), Y(1) with Lazy Propagation\n",
    "    from the inputed Baysian Network as a pandas Data Frame couple.\n",
    "    \"\"\"\n",
    "\n",
    "    ie = gum.LazyPropagation(bn)\n",
    "\n",
    "    ie.setEvidence({\"T\": 0})\n",
    "    ie.makeInference()\n",
    "    var_labels = list()\n",
    "    var = ie.posterior(\"Y\").variable(0)\n",
    "    for i in range(var.domainSize()):\n",
    "        var_labels.append(var.label(i))\n",
    "    Y0 = pd.DataFrame({\"T\": 0, \"interval\": var_labels,\n",
    "                       \"probability\": ie.posterior(\"Y\").tolist()})\n",
    "    Y0[\"interval_mean\"] = Y0[\"interval\"].apply(getStringIntervalMean)\n",
    "\n",
    "    ie.setEvidence({\"T\": 1})\n",
    "    ie.makeInference()\n",
    "    var_labels = list()\n",
    "    var = ie.posterior(\"Y\").variable(0)\n",
    "    for i in range(var.domainSize()):\n",
    "        var_labels.append(var.label(i))\n",
    "    Y1 = pd.DataFrame({\"T\": 1, \"interval\": var_labels,\n",
    "                       \"probability\": ie.posterior(\"Y\").tolist()})\n",
    "    Y1[\"interval_mean\"] = Y1[\"interval\"].apply(getStringIntervalMean)\n",
    "\n",
    "    return (Y0, Y1)\n",
    "\n",
    "def getTau(Y : tuple[pd.DataFrame]) -> float:\n",
    "    \"\"\"\n",
    "    Returns estimation of the ATE tau from pandas Data Frame couple\n",
    "    (Y(0), Y(1)).\n",
    "    \"\"\"\n",
    "\n",
    "    E0 = (Y[0][\"interval_mean\"] * Y[0][\"probability\"]).sum()\n",
    "    E1 = (Y[1][\"interval_mean\"] * Y[1][\"probability\"]).sum()\n",
    "    tau = E1 - E0\n",
    "\n",
    "    return tau\n",
    "\n",
    "def plotResults(Y_hat : pd.DataFrame, Y : pd.DataFrame,\n",
    "                plot_title : str) -> None:\n",
    "    \"\"\"\n",
    "    Scatters Y_hat data and plots Y data in a plot titled plot_title.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.scatter(x=Y_hat[0][\"interval_mean\"] ,y=Y_hat[0][\"probability\"],\n",
    "                color=\"tab:blue\", label=\"$\\hat{Y}(0)$\", s=10)\n",
    "    plt.scatter(x=Y_hat[1][\"interval_mean\"] ,y=Y_hat[1][\"probability\"],\n",
    "                color=\"tab:orange\", label=\"$\\hat{Y}(1)$\", s=10)\n",
    "    plt.plot(Y[\"y0\"], color=\"tab:blue\", label=\"Y(0)\")\n",
    "    plt.plot(Y[\"y1\"], color=\"tab:orange\", label=\"Y(1)\")\n",
    "    plt.title(plot_title)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBN(# Covariate parameters\n",
    "          covariate_start : int = -3.0,\n",
    "          covariate_end : int = 5.0 ,\n",
    "          covariate_num_split : int = 10,\n",
    "          covariate_distribution = None,\n",
    "          # Outcome parameters\n",
    "          outcome_start = -20.0 ,\n",
    "          outcome_end = 40.0 ,\n",
    "          outcome_num_split = 60,\n",
    "          outcome_loc_expr : str | None = None,\n",
    "          # Other\n",
    "          data : pd.DataFrame | None = None,\n",
    "          add_arcs : bool = True) -> gum.BayesNet:\n",
    "    \"\"\"\n",
    "    Returns Baysian Network corresponding to the model by discretising\n",
    "    countinous variables with given parameters.\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        bn = gum.BayesNet()\n",
    "        for i in range(1,5):\n",
    "            bn.add(f\"X{i}[{covariate_start}:{covariate_end}:{covariate_num_split}]\")\n",
    "        bn.add(\"T[2]\")\n",
    "        bn.add(f\"Y[{outcome_start}:{outcome_end}:{outcome_num_split}]\")\n",
    "\n",
    "    else :\n",
    "        disc = skbn.BNDiscretizer(defaultDiscretizationMethod=\"uniform\",\n",
    "                                  defaultNumberOfBins=covariate_num_split)\n",
    "        disc.setDiscretizationParameters(\"T\", 'NoDiscretization', [0, 1])\n",
    "        disc.setDiscretizationParameters(\"Y\", 'uniform', outcome_num_split)\n",
    "        bn = disc.discretizedBN(data)\n",
    "\n",
    "    if add_arcs :\n",
    "        bn.beginTopologyTransformation()\n",
    "        for _, name in bn:\n",
    "            if name != \"Y\":\n",
    "                bn.addArc(name, \"Y\")\n",
    "        bn.endTopologyTransformation()\n",
    "\n",
    "    if covariate_distribution is not None :\n",
    "        bn.cpt(\"T\").fillWith([0.5, 0.5])\n",
    "        for i in range(1,5):\n",
    "            bn.cpt(f\"X{i}\").fillFromDistribution(covariate_distribution)\n",
    "\n",
    "    if outcome_loc_expr is not None:\n",
    "        bn.cpt(\"Y\").fillFromDistribution(norm, loc=outcome_loc_expr, scale=1)\n",
    "\n",
    "    return bn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By employing a 10-bin discretization for the covariates and a 60-bin discretization for the outcome, the Bayesian Network estimator accurately approximates the true distribution for both the treated and untreated outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_exbn = getBN(covariate_distribution=norm(loc=1, scale=1),\n",
    "                 outcome_loc_expr=lin_expr)\n",
    "nl_exbn  = getBN(covariate_distribution=norm(loc=1, scale=1),\n",
    "                 outcome_loc_expr=nl_expr)\n",
    "print(lin_exbn)\n",
    "\n",
    "gnb.sideBySide(gnb.getInference(lin_exbn, evs={\"T\":0}, size=\"10\"),\n",
    "               gnb.getInference(lin_exbn, evs={\"T\":1}, size=\"10\"),\n",
    "               captions=[\"Linear Y(0)\", \"Linear Y(1)\"]) \n",
    "\n",
    "gnb.sideBySide(gnb.getInference(nl_exbn, evs={\"T\":0}, size=\"10\"),\n",
    "               gnb.getInference(nl_exbn, evs={\"T\":1}, size=\"10\"),\n",
    "               captions=[\"Non-Linear Y(0)\", \"Non-Linear Y(1)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, obtaining the average treatment effect is straightforward using pyAgrum's LazyPropagation exact inference. This involves setting the evidence of the treatment $T$ to $0$ and then to $1$ to compute the respective posterior CPTs. The ATE is subsequently obtained by performing manipulations on the difference between the CPTs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ATE(bn : gum.BayesNet) -> float:\n",
    "    \"\"\"\n",
    "    Returns estimation of the ATE directly from Baysian Network.\n",
    "    \"\"\"\n",
    "    ie = gum.LazyPropagation(bn)\n",
    "\n",
    "    ie.setEvidence({\"T\": 0})\n",
    "    ie.makeInference()\n",
    "    p0 = ie.posterior(\"Y\")\n",
    "\n",
    "    ie.chgEvidence(\"T\",1)\n",
    "    ie.makeInference()\n",
    "    p1 = ie.posterior(\"Y\")\n",
    "\n",
    "    dif = p1 - p0\n",
    "    return dif.expectedValue(lambda d: dif.variable(0).numerical(d[dif.variable(0).name()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lin_exbn)\n",
    "print(f\"{ATE(lin_exbn) = }\")\n",
    "print(f\"\\n{nl_exbn}\")    \n",
    "print(f\"{ATE(nl_exbn) = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how the fineness of covariate discretization impacts the outcome distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_split_list = [2, 5, 10]\n",
    "\n",
    "plt.subplots(figsize=(7, 3.5*len(num_split_list)))\n",
    "\n",
    "for i in range(len(num_split_list)):\n",
    "\n",
    "    covariate_num_split = num_split_list[i]\n",
    "\n",
    "    # Linear Model\n",
    "\n",
    "    plt.subplot(len(num_split_list), 2, 2*i+1)\n",
    "    lin_ex_bn = getBN(covariate_num_split=num_split_list[i],\n",
    "                      covariate_distribution=norm(loc=1, scale=1),\n",
    "                      outcome_loc_expr=lin_expr)\n",
    "    lin_Y_hat = getY(lin_ex_bn)\n",
    "    plotResults(lin_Y_hat, lin_pdf_df,\n",
    "                f\"Linear Model Resulting $Y$ distributions \\n\" \\\n",
    "                f\"$X_i$ splits: {covariate_num_split}, \" \\\n",
    "                f\"ATE: {getTau(lin_Y_hat)}\")\n",
    "\n",
    "    # Non Linear Model\n",
    "\n",
    "    plt.subplot(len(num_split_list), 2, 2*i+2)\n",
    "    nl_ex_bn = getBN(covariate_num_split=num_split_list[i],\n",
    "                     covariate_distribution=norm(loc=1, scale=1),\n",
    "                     outcome_loc_expr=nl_expr)\n",
    "    nl_Y_hat = getY(nl_ex_bn)\n",
    "    plotResults(nl_Y_hat, nl_pdf_df,\n",
    "                f\"Non-Linear Model Resulting $Y$ distributions \\n\" \\\n",
    "                f\"$X_i$ splits: {covariate_num_split}, \" \\\n",
    "                f\"ATE: {getTau(nl_Y_hat)}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finer discretization with a greater number of bins results in improved approximations of the probability density functions. However, despite the use of rough discretization, the estimations of the ATE remain remarkably accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Parameter Learning\n",
    "\n",
    "Given the data generating function defined above, parameter learning methods can be employed to infer the underlying distribution based on the given structure of the Bayesian network. The default Mutual Information based Inference of Causal Networks (MIIC) algorithm utilized  in the `BNLearner` class effectively performs this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Model\n",
    "\n",
    "lin_template = getBN(data=lin_df)\n",
    "\n",
    "lin_p_learner = gum.BNLearner(lin_df, lin_template)\n",
    "lin_p_learner.useNMLCorrection()\n",
    "lin_p_learner.useSmoothingPrior(1e-6)\n",
    "\n",
    "lin_plbn = gum.BayesNet(lin_template)\n",
    "lin_p_learner.fitParameters(lin_plbn)\n",
    "\n",
    "# Non-Linear Model\n",
    "\n",
    "nl_template = getBN(data=nl_df)\n",
    "\n",
    "nl_p_learner = gum.BNLearner(nl_df, nl_template)\n",
    "nl_p_learner.useNMLCorrection()\n",
    "nl_p_learner.useSmoothingPrior(1e-6)\n",
    "\n",
    "nl_plbn = gum.BayesNet(nl_template)\n",
    "nl_p_learner.fitParameters(nl_plbn)\n",
    "\n",
    "print(lin_p_learner)\n",
    "gnb.sideBySide(gnb.getInference(lin_plbn, evs={\"T\":0}, size=\"10\"),\n",
    "               gnb.getInference(lin_plbn, evs={\"T\":1}, size=\"10\"),\n",
    "               captions=[\"Linear Y(0)\", \"Linear Y(1)\"])\n",
    "\n",
    "print(nl_p_learner)\n",
    "gnb.sideBySide(gnb.getInference(nl_plbn, evs={\"T\":0}, size=\"10\"),\n",
    "               gnb.getInference(nl_plbn, evs={\"T\":1}, size=\"10\"),\n",
    "               captions=[\"Non-Linear Y(0)\", \"Non-Linear Y(1)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the inferred outcome distribution generally matches the exact distribution. However, the ATE seems to be biased, as it is consistently smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(7, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "lin_Y = getY(lin_plbn)\n",
    "plotResults(lin_Y, lin_pdf_df,\n",
    "            f\"Linear Model Resulting $Y$ distributions \\n\" \\\n",
    "            f\"$X_i$ splits: {covariate_num_split}, ATE: {getTau(lin_Y)}\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "nl_Y = getY(nl_plbn)\n",
    "plotResults(nl_Y, nl_pdf_df,\n",
    "            f\"Non-Linear Model Resulting $Y$ distributions \\n\" \\\n",
    "            f\"$X_i$ splits: {covariate_num_split}, ATE: {getTau(nl_Y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This underestimation can be further observed with varying numbers of observations in both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_tau_hat_arr = list()\n",
    "nl_tau_hat_arr = list()\n",
    "\n",
    "num_obs_list = [5000, 10000, 20000, 40000]\n",
    "num_shots = 10\n",
    "\n",
    "for i in num_obs_list:\n",
    "\n",
    "    lin_tau_hat_arr.append(list())\n",
    "    nl_tau_hat_arr.append(list())\n",
    "\n",
    "    for j in range(num_shots):\n",
    "\n",
    "        lin_df = linear_simulation(i, 1.0)\n",
    "        nl_df = non_linear_simulation(i, 1.0)\n",
    "\n",
    "        discretizer = skbn.BNDiscretizer(\"uniform\", 30)\n",
    "\n",
    "        # Linear Model\n",
    "\n",
    "        lin_template = getBN(data=lin_df)\n",
    "\n",
    "        lin_p_learner = gum.BNLearner(lin_df, lin_template)\n",
    "        lin_p_learner.useNMLCorrection()\n",
    "        lin_p_learner.useSmoothingPrior(1e-6)\n",
    "        lin_p_learner.setSliceOrder([[\"T\"],[\"X1\",\"X2\",\"X3\",\"X4\"],[\"Y\"]])\n",
    "\n",
    "        lin_plbn = gum.BayesNet(lin_template)\n",
    "        lin_p_learner.fitParameters(lin_plbn)\n",
    "\n",
    "        lin_Y_hat = getY(lin_plbn)\n",
    "        lin_tau_hat = getTau(lin_Y_hat)\n",
    "        lin_tau_hat_arr[-1].append(lin_tau_hat)\n",
    "\n",
    "        # Non-Linear Model\n",
    "\n",
    "        nl_template = getBN(data=nl_df)\n",
    "\n",
    "        nl_p_learner = gum.BNLearner(nl_df, nl_template)\n",
    "        nl_p_learner.useNMLCorrection()\n",
    "        nl_p_learner.useSmoothingPrior(1e-6)\n",
    "        nl_p_learner.setSliceOrder([[\"T\"],[\"X1\",\"X2\",\"X3\",\"X4\"],[\"Y\"]])\n",
    "\n",
    "        nl_plbn = gum.BayesNet(nl_template)\n",
    "        nl_p_learner.fitParameters(nl_plbn)\n",
    "\n",
    "        nl_Y_hat = getY(nl_plbn)\n",
    "        nl_tau_hat = getTau(nl_Y_hat)\n",
    "        nl_tau_hat_arr[-1].append(nl_tau_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(7, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.boxplot(lin_tau_hat_arr, labels=num_obs_list, meanline=False,\n",
    "            showmeans=True, showcaps=True)\n",
    "plt.axhline(y=10, color='r', linestyle='--', linewidth=1)\n",
    "plt.title(f\"Linear Model ATE evolution ({num_shots} runs)\")\n",
    "plt.xlabel(\"Number of observations\")\n",
    "plt.ylabel(\"$\\hat{ATE}$\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.boxplot(nl_tau_hat_arr, labels=num_obs_list, meanline=False,\n",
    "            showmeans=True, showcaps=True)\n",
    "plt.axhline(y=10, color='r', linestyle='--', linewidth=1)\n",
    "plt.title(f\"Non Linear Model ATE evolution ({num_shots} runs)\")\n",
    "plt.xlabel(\"Number of observations\")\n",
    "plt.ylabel(\"$\\hat{ATE}$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With an increasing number of observations, we observe a convergence of the estimation towards the true value of the ATE and a corresponding reduction in variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Structure Learning\n",
    "\n",
    "The network's structure and the distributions of the variables can be derived from a sufficiently large dataset through non-parametric learning methods. However, to ensure the integrity of the process, we will impose a slice order on the learner. This ensures that no node is an ancestor of the treatment variable, and no node is a descendant of the outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_template = getBN(data=lin_df)\n",
    "lin_s_learner = gum.BNLearner(lin_df, lin_template)\n",
    "lin_s_learner.useNMLCorrection()\n",
    "lin_s_learner.useSmoothingPrior(1e-6)\n",
    "lin_s_learner.setSliceOrder([[\"T\"],[\"X1\",\"X2\",\"X3\",\"X4\"],[\"Y\"]])\n",
    "lin_slbn = lin_s_learner.learnBN()\n",
    "\n",
    "nl_template = getBN(data=nl_df)\n",
    "nl_s_learner = gum.BNLearner(nl_df, nl_template)\n",
    "nl_s_learner.useNMLCorrection()\n",
    "nl_s_learner.useSmoothingPrior(1e-6)\n",
    "nl_s_learner.setSliceOrder([[\"T\"],[\"X1\",\"X2\",\"X3\",\"X4\"],[\"Y\"]])\n",
    "nl_slbn = nl_s_learner.learnBN()\n",
    "\n",
    "print(lin_s_learner)\n",
    "gnb.sideBySide(gnb.getInference(lin_slbn, evs={\"T\":0}, size=\"10\"),\n",
    "               gnb.getInference(lin_slbn, evs={\"T\":1}, size=\"10\"),\n",
    "               captions=[\"Linear Y(0)\", \"Linear Y(1)\"])\n",
    "\n",
    "print(nl_s_learner)\n",
    "gnb.sideBySide(gnb.getInference(nl_slbn, evs={\"T\":0}, size=\"10\"),\n",
    "               gnb.getInference(nl_slbn, evs={\"T\":1}, size=\"10\"),\n",
    "               captions=[\"Non-Linear Y(0)\", \"Non-Linear Y(1)\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 10,000 observations, structure learning yields a more accurate estimation of the Average Treatment Effect (ATE) compared to parameter learning. This improvement can be attributed to the use of a less complex structure, as opposed to the structure used previously, which features a higher in-degree on the outcome node. The increased number of parameters to be estimated in the outcome model due to this higher in-degree is suboptimal for smaller datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(7, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "lin_Y = getY(lin_slbn)\n",
    "plotResults(lin_Y, lin_pdf_df,\n",
    "            f\"Linear Model Resulting $Y$ distributions \\n\" \\\n",
    "            f\"$X_i$ splits: {covariate_num_split}, ATE: {getTau(lin_Y)}\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "nl_Y = getY(nl_slbn)\n",
    "plotResults(nl_Y, nl_pdf_df,\n",
    "            f\"Non-Linear Model Resulting $Y$ distributions \\n\" \\\n",
    "            f\"$X_i$ splits: {covariate_num_split}, ATE: {getTau(nl_Y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_tau_hat_arr = []\n",
    "nl_tau_hat_arr = []\n",
    "\n",
    "num_obs_list = [5000, 10000, 20000, 40000]\n",
    "num_shots = 10\n",
    "\n",
    "for i in num_obs_list:\n",
    "    lin_tau_hat_arr.append(list())\n",
    "    nl_tau_hat_arr.append(list())\n",
    "    for j in range(num_shots):\n",
    "\n",
    "        lin_df = linear_simulation(i, 1.0)\n",
    "        nl_df = non_linear_simulation(i, 1.0)\n",
    "\n",
    "        discretizer = skbn.BNDiscretizer(\"uniform\", 30)\n",
    "\n",
    "        lin_template = discretizer.discretizedBN(lin_df)\n",
    "        lin_struct_learner = gum.BNLearner(lin_df, lin_template)\n",
    "        lin_slbn = lin_struct_learner.learnBN()\n",
    "\n",
    "        nl_template = discretizer.discretizedBN(nl_df)\n",
    "        nl_struct_learner = gum.BNLearner(nl_df, nl_template)\n",
    "        nl_slbn = nl_struct_learner.learnBN()\n",
    "\n",
    "        lin_Y_hat = getY(lin_slbn)\n",
    "        lin_tau_hat = getTau(lin_Y_hat)\n",
    "        lin_tau_hat_arr[-1].append(lin_tau_hat)\n",
    "\n",
    "        nl_Y_hat = getY(nl_slbn)\n",
    "        nl_tau_hat = getTau(nl_Y_hat)\n",
    "        nl_tau_hat_arr[-1].append(nl_tau_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(7, 3))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.boxplot(lin_tau_hat_arr, labels=num_obs_list, meanline=False,\n",
    "            showmeans=True, showcaps=True)\n",
    "plt.axhline(y=10, color='r', linestyle='--', linewidth=1)\n",
    "plt.title(f\"Linear Model ATE evolution ({num_shots} runs)\")\n",
    "plt.xlabel(\"Number of observations\")\n",
    "plt.ylabel(\"$\\hat{ATE}$\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "\n",
    "plt.boxplot(nl_tau_hat_arr, labels=num_obs_list, meanline=False,\n",
    "            showmeans=True, showcaps=True)\n",
    "plt.axhline(y=10, color='r', linestyle='--', linewidth=1)\n",
    "plt.title(f\"Non Linear Model ATE evolution ({num_shots} runs)\")\n",
    "plt.xlabel(\"Number of observations\")\n",
    "plt.ylabel(\"$\\hat{ATE}$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After evaluating various estimation methods using generated data, we will now direct our attention to real data from the Tennessee Student/Teacher Achievement Ratio (STAR) trial. This randomized controlled trial, initiated in 1985, is a pioneering study in the field of education, designed to assess the effects of smaller class sizes in primary schools (T) on students' academic performance (Y). \n",
    "\n",
    "The covariates in this study include:\n",
    "\n",
    "* `gender`\n",
    "* `age`\n",
    "* `g1freelunch` being the number of lunchs provided to the child per day\n",
    "* `g1surban` the localisation of the school (inner city or rural)\n",
    "* `ethnicity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# Load data - read everything as a string and then cast\n",
    "star_df = pd.read_csv(\"./STAR_data.csv\", sep=\",\", dtype=str)\n",
    "star_df = star_df.rename(columns={\"race\": \"ethnicity\"})\n",
    "\n",
    "# Fill na\n",
    "star_df = star_df.fillna({\"g1freelunch\": 0, \"g1surban\": 0})\n",
    "drop_star_l = [\"g1tlistss\", \"g1treadss\", \"g1tmathss\", \"g1classtype\",\n",
    "\"birthyear\", \"birthmonth\", \"birthday\", \"gender\",\n",
    "\"ethnicity\", \"g1freelunch\", \"g1surban\"]\n",
    "star_df = star_df.dropna(subset=drop_star_l, how='any')\n",
    "\n",
    "# Cast value types before processing\n",
    "star_df[\"gender\"] = star_df[\"gender\"].astype(int)\n",
    "star_df[\"ethnicity\"] = star_df[\"ethnicity\"].astype(int)\n",
    "\n",
    "star_df[\"g1freelunch\"] = star_df[\"g1freelunch\"].astype(int)\n",
    "star_df[\"g1surban\"] = star_df[\"g1surban\"].astype(int)\n",
    "star_df[\"g1classtype\"] = star_df[\"g1classtype\"].astype(int)\n",
    "\n",
    "# Keep only class type 1 and 2 (in the initial trial,\n",
    "# 3 class types where attributed and the third one was big classes\n",
    "# but with a teaching assistant)\n",
    "star_df = star_df[~(star_df[\"g1classtype\"] == 3)].reset_index(drop=True)\n",
    "\n",
    "# Compute the outcome\n",
    "star_df[\"Y\"] = (star_df[\"g1tlistss\"].astype(int) +\n",
    "                star_df[\"g1treadss\"].astype(int) +\n",
    "                star_df[\"g1tmathss\"].astype(int)) / 3\n",
    "\n",
    "# Compute the treatment\n",
    "star_df[\"T\"] = star_df[\"g1classtype\"].apply(lambda x: 0 if x == 2 \\\n",
    "                                                        else 1)\n",
    "\n",
    "# Transform date to obtain age (Notice: if na --> date is NaT)\n",
    "star_df[\"date\"] = pd.to_datetime(star_df[\"birthyear\"] + \"/\"\n",
    "+ star_df[\"birthmonth\"] + \"/\"\n",
    "+ star_df[\"birthday\"], yearfirst=True, errors=\"coerce\")\n",
    "star_df[\"age\"] = (np.datetime64(\"1985-01-01\") - star_df[\"date\"])\n",
    "star_df[\"age\"] = star_df[\"age\"].dt.days / 365.25\n",
    "\n",
    "# Keep only covariates we consider predictive of the outcome\n",
    "star_covariates_l = [\"gender\", \"ethnicity\", \"age\",\n",
    "                     \"g1freelunch\", \"g1surban\"]\n",
    "star_df = star_df[[\"Y\", \"T\"] + star_covariates_l]\n",
    "\n",
    "# Map numerical to categorical\n",
    "star_df[\"gender\"] = star_df[\"gender\"].apply(lambda x: \"Girl\" if x == 2 \\\n",
    "                                            else \"Boy\").astype(\"category\")\n",
    "star_df[\"ethnicity\"] = star_df[\"ethnicity\"].map( \\\n",
    "    {1:\"White\", 2:\"Black\", 3:\"Asian\",\n",
    "     4:\"Hispanic\",5:\"Nat_American\", 6:\"Other\"}).astype(\"category\")\n",
    "star_df[\"g1surban\"] = star_df[\"g1surban\"].map( \\\n",
    "    {1:\"Inner_city\", 2:\"Suburban\",\n",
    "     3:\"Rural\", 4:\"Urban\"}).astype(\"category\")\n",
    "\n",
    "star_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toHist(X : list[float], Y : list[float],\n",
    "           n : int = 1000) -> list[float]:\n",
    "    \"\"\"\n",
    "    Transforms X, Y data from plotting to a\n",
    "    histogram plotting format\n",
    "    \"\"\"\n",
    "    res = list()\n",
    "    for i in range(len(X)):\n",
    "        res += [X[i]]*int(n*Y[i])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Structure Learning\n",
    "\n",
    "n the absence of prior knowledge regarding the underlying distributions of the variables and their relationships, causal inference can be challenging. Consequently, we will first utilize structure learning to automatically identify the network's underlying structure. To assist the learning process, we will impose a slice order on the variables once again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = skbn.BNDiscretizer(defaultDiscretizationMethod='uniform')\n",
    "disc.setDiscretizationParameters(\"age\", 'uniform', 24)\n",
    "disc.setDiscretizationParameters(\"Y\", 'uniform', 30)\n",
    "\n",
    "template = disc.discretizedBN(star_df)\n",
    "\n",
    "learner = gum.BNLearner(star_df, template)\n",
    "learner.useNMLCorrection()\n",
    "learner.useSmoothingPrior(1e-6)\n",
    "learner.setSliceOrder([[\"T\", \"ethnicity\", \"gender\", \"age\"],\n",
    "                       [\"g1surban\", \"g1freelunch\", ], [\"Y\"]])\n",
    "star_slbn = learner.learnBN()\n",
    "\n",
    "print(learner)\n",
    "\n",
    "gnb.sideBySide(gexpl.getInformation(star_slbn, size=\"50\"),\n",
    "               gnb.getInference(star_slbn, size=\"50\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This initial approach appears promising, as the inferred causal relationships are somewhat consistent with what might be expected from an non-expert perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = getY(star_slbn)\n",
    "\n",
    "x0, y0 = (Y_hat[0][\"interval_mean\"].to_numpy(),\n",
    "          Y_hat[0][\"probability\"].to_numpy())\n",
    "x1, y1 = (Y_hat[1][\"interval_mean\"].to_numpy(),\n",
    "          Y_hat[1][\"probability\"].to_numpy())\n",
    "\n",
    "density0 = gaussian_kde(toHist(x0, y0))\n",
    "density1 = gaussian_kde(toHist(x1, y1))\n",
    "xs = np.linspace(400,700,200)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "\n",
    "plt.plot(xs, density0(xs), color=\"tab:blue\", label=\"$\\hat{Y}(0)$\")\n",
    "\n",
    "plt.plot(xs, density1(xs), color=\"tab:orange\", label=\"$\\hat{Y}(0)$\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Resulting $Y$ distributions\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Estimated ATE : {getTau(Y_hat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe a slight change in the outcome distribution. However, since the outcome takes values in the hundreds, this results in a non-negligeable impact on the treatment effect, given that the outcome is defined as the average of the students' three grades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Parameter Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using different structures when conducting parameter learning can yield varying results. For the sake of illustration, we will examine how the estimation performs when arcs from the `age` and `gender` covariates are added to the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = skbn.BNDiscretizer(defaultDiscretizationMethod='uniform')\n",
    "disc.setDiscretizationParameters(\"age\", 'uniform', 24)\n",
    "disc.setDiscretizationParameters(\"Y\", 'uniform', 30)\n",
    "\n",
    "template = disc.discretizedBN(star_df)\n",
    "\n",
    "learner = gum.BNLearner(star_df, template)\n",
    "learner.useNMLCorrection()\n",
    "learner.useSmoothingPrior(1e-6)\n",
    "\n",
    "star_plbn = gum.BayesNet(template)\n",
    "star_plbn.addArc(\"T\",\"Y\")\n",
    "star_plbn.addArc(\"ethnicity\",\"g1surban\")\n",
    "star_plbn.addArc(\"ethnicity\",\"g1freelunch\")\n",
    "star_plbn.addArc(\"g1surban\",\"g1freelunch\")\n",
    "star_plbn.addArc(\"g1freelunch\",\"Y\")\n",
    "star_plbn.addArc(\"gender\",\"Y\")\n",
    "star_plbn.addArc(\"age\",\"Y\")\n",
    "\n",
    "learner.fitParameters(star_plbn)\n",
    "\n",
    "print(learner)\n",
    "\n",
    "gnb.sideBySide(gexpl.getInformation(star_plbn, size=\"50\"),\n",
    "               gnb.getInference(star_plbn, size=\"50\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = getY(star_plbn)\n",
    "\n",
    "x0, y0 = (Y_hat[0][\"interval_mean\"].to_numpy(),\n",
    "          Y_hat[0][\"probability\"].to_numpy())\n",
    "x1, y1 = (Y_hat[1][\"interval_mean\"].to_numpy(),\n",
    "          Y_hat[1][\"probability\"].to_numpy())\n",
    "\n",
    "density0 = gaussian_kde(toHist(x0, y0))\n",
    "density1 = gaussian_kde(toHist(x1, y1))\n",
    "xs = np.linspace(400,700,200)\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "\n",
    "plt.plot(xs, density0(xs), color=\"tab:blue\", label=\"$\\hat{Y}(0)$\")\n",
    "\n",
    "plt.plot(xs, density1(xs), color=\"tab:orange\", label=\"$\\hat{Y}(0)$\")\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.title(\"Resulting $Y$ distributions\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Estimated ATE : {getTau(Y_hat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As anticipated, there are observable differences between the parameter learning method and the structure learning method. When compared to direct estimation methods, such as the Difference in Means (DM) estimator and the Ordinary Least Squares (OLS) estimator, which yield average treatment effects of 12.81 and 10.77, respectively, our findings remain largely consistent. These results suggest that incorporating age and gender variables into the outcome model may deteriorate the final estimation accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
